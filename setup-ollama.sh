#!/bin/bash

# Script para configurar Ollama con MemoAI
echo "ðŸ¤– Configurando Ollama para MemoAI..."

# 1. Verificar si Ollama estÃ¡ instalado
if ! command -v ollama &> /dev/null; then
    echo "âŒ Ollama no estÃ¡ instalado. Instalando..."
    
    # Instalar Ollama
    curl -fsSL https://ollama.ai/install.sh | sh
    
    # Iniciar servicio Ollama
    sudo systemctl start ollama
    sudo systemctl enable ollama
else
    echo "âœ… Ollama ya estÃ¡ instalado"
fi

# 2. Verificar que Ollama estÃ© ejecutÃ¡ndose
echo "ðŸ” Verificando estado de Ollama..."
if ! pgrep -f ollama > /dev/null; then
    echo "â–¶ï¸ Iniciando Ollama..."
    sudo systemctl start ollama
    sleep 5
fi

# 3. Descargar modelo recomendado
echo "ðŸ“¥ Descargando modelo llama3.2:latest..."
ollama pull llama3.2:latest

# 4. Verificar que el modelo estÃ© disponible
echo "ðŸ” Verificando modelos disponibles..."
ollama list

# 5. Probar conexiÃ³n
echo "ðŸ§ª Probando conexiÃ³n con Ollama..."
curl -f http://localhost:11434/api/tags || echo "âŒ No se pudo conectar a Ollama"

# 6. Crear archivo .env si no existe
if [ ! -f .env ]; then
    echo "ðŸ“ Creando archivo .env..."
    if [ -f config.example.env ]; then
        cp config.example.env .env
        echo "âœ… Archivo .env creado desde config.example.env"
    else
        cat > .env << EOF
# MemoAI Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:latest
NODE_ENV=production
PORT=9021
EOF
        echo "âœ… Archivo .env creado"
    fi
else
    echo "âœ… Archivo .env ya existe"
fi

echo "ðŸŽ‰ ConfiguraciÃ³n de Ollama completada!"
echo "ðŸŒ Ollama estÃ¡ disponible en: http://localhost:11434"
echo "ðŸ¤– Modelo configurado: llama3.2:latest"
echo ""
echo "Para usar Ollama con MemoAI:"
echo "1. AsegÃºrate de que Ollama estÃ© ejecutÃ¡ndose: sudo systemctl start ollama"
echo "2. Inicia MemoAI: docker compose up -d"
echo "3. La aplicaciÃ³n usarÃ¡ Ollama automÃ¡ticamente si estÃ¡ disponible"
